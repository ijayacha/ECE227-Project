{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3a9bef",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15506aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from random import uniform, seed\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02119d",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BarabÃ¡si-Albert graph data\n",
    "b_node_file = \"barabasi_100_nodes.csv\"\n",
    "b_edge_file = \"barabasi_100_edges.csv\"\n",
    "\n",
    "nodes = pd.read_csv(b_node_file)\n",
    "edges = pd.read_csv(b_edge_file)\n",
    "\n",
    "# Graph\n",
    "BG = nx.from_pandas_edgelist(edges, source='source', target='target')\n",
    "BG.add_nodes_from(nodes['id'])\n",
    "\n",
    "# Load the Erdos-Renyi graph data\n",
    "e_node_file = \"erdos_100_nodes.csv\"\n",
    "e_edge_file = \"erdos_100_edges.csv\"\n",
    "\n",
    "nodes = pd.read_csv(e_node_file)\n",
    "edges = pd.read_csv(e_edge_file)\n",
    "\n",
    "# Graph\n",
    "EG = nx.from_pandas_edgelist(edges, source='source', target='target')\n",
    "EG.add_nodes_from(nodes['id'])\n",
    "\n",
    "\n",
    "# Load the Facebook pages graph data\n",
    "fb_node_file = \"fb-pages-food_nodes.csv\"\n",
    "fb_edge_file = \"fb-pages-food_edges.csv\"\n",
    "\n",
    "nodes = pd.read_csv(fb_node_file)\n",
    "edges = pd.read_csv(fb_edge_file)\n",
    "\n",
    "# Graph\n",
    "fbG = nx.from_pandas_edgelist(edges, source='source', target='target')\n",
    "fbG.add_nodes_from(nodes['new_id'])\n",
    "\n",
    "# Load data\n",
    "edges = pd.read_csv(\"txs_edgelist.csv\")\n",
    "\n",
    "# Adjust column names here based on what the CSV shows\n",
    "G_full = nx.from_pandas_edgelist(edges, source='txId1', target='txId2', create_using=nx.Graph())\n",
    "\n",
    "# Sample 500 nodes\n",
    "sub_nodes = list(G_full.nodes)[:500]\n",
    "BcG = G_full.subgraph(sub_nodes).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb7bb5",
   "metadata": {},
   "source": [
    "## Functions for influence propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a588bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_threshold(sfg, seeds, threshold_range=(0.0, 1.0)):\n",
    "    \"\"\"\n",
    "    Simulate the Linear Threshold Model.\n",
    "    G: NetworkX graph\n",
    "    seeds: initial active nodes (seed nodes)\n",
    "    threshold_range: range of thresholds for each node\n",
    "    Returns the set of activated nodes.\n",
    "    \"\"\"\n",
    "    # Assign random thresholds to each node\n",
    "    thresholds = {node: np.random.uniform(*threshold_range) for node in sfg.nodes()}\n",
    "    activated = set(seeds)\n",
    "    while True:\n",
    "        new_activated = set()\n",
    "        for node in sfg.nodes():\n",
    "            if node not in activated:\n",
    "                neighbors = list(sfg.neighbors(node))\n",
    "                if not neighbors:\n",
    "                    continue\n",
    "                active_neighbors = sum(1 for neighbor in neighbors if neighbor in activated)\n",
    "                if (active_neighbors / len(neighbors)) >= thresholds[node]:\n",
    "                    new_activated.add(node)\n",
    "        if not new_activated:\n",
    "            break\n",
    "        activated.update(new_activated)\n",
    "    return activated\n",
    "\n",
    "def Independent_Cascade_model(G, seeds, influence_probabilities):\n",
    "    activated = set(seeds)\n",
    "    prev_round_activated = activated.copy()\n",
    "    ctr = 0\n",
    "    node_idx_dict = {x: idx for idx, x in enumerate(G.nodes)}\n",
    "    while True:\n",
    "        ctr += 1\n",
    "        new_activated = set()\n",
    "        modified_influence_probabilities = np.delete(influence_probabilities,[node_idx_dict[x] for x in activated],axis=1)\n",
    "        for node in prev_round_activated:\n",
    "            neighbors = set(G.neighbors(node))-activated\n",
    "            node_influence_probabilities = modified_influence_probabilities[node_idx_dict[node],:]\n",
    "            node_influence_probabilities = node_influence_probabilities[node_influence_probabilities!=0]\n",
    "            \n",
    "            new_activated = new_activated.union(set(list(np.array(list(neighbors))[np.random.uniform(size=len(neighbors))<node_influence_probabilities])))\n",
    "\n",
    "        activated.update(new_activated)\n",
    "        if not new_activated:\n",
    "          break\n",
    "\n",
    "        prev_round_activated = new_activated\n",
    "\n",
    "    return activated\n",
    "\n",
    "def Uniform_influence_prob(G, p):\n",
    "    return p*(nx.adjacency_matrix(G).toarray())\n",
    "\n",
    "def Weighted_cascade(G):\n",
    "    adj_mat = nx.adjacency_matrix(G).toarray()\n",
    "    return np.divide(adj_mat.T,np.sum(adj_mat,axis=1)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c732f",
   "metadata": {},
   "source": [
    "## Greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1528317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_optimizer(G, k, spread_fn):\n",
    "    selected = []\n",
    "    for _ in range(k):\n",
    "        best_node = None\n",
    "        best_spread = -1\n",
    "        for node in G.nodes:\n",
    "            if node in selected:\n",
    "                continue\n",
    "            spread = spread_fn(G, selected + [node])\n",
    "            if spread > best_spread:\n",
    "                best_spread = spread\n",
    "                best_node = node\n",
    "        selected.append(best_node)\n",
    "    return selected\n",
    "\n",
    "# Spread Estimation Functions\n",
    "def estimate_spread_ic(G, seeds, simulations=50):\n",
    "    return np.mean([len(Independent_Cascade_model(G, seeds, influence_probabilities)) for _ in range(simulations)])\n",
    "\n",
    "def estimate_spread_lt(G, seeds, simulations=50):\n",
    "    return np.mean([len(linear_threshold(G, seeds)) for _ in range(simulations)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644fe0a4",
   "metadata": {},
   "source": [
    "## Reverse Influence Sampling Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea3e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RRS(Graph,p):   \n",
    "    \"\"\"\n",
    "    Inputs: Graph:  NetworkX graph\n",
    "            p:  Disease propagation probability\n",
    "    Return: A random reverse reachable set expressed as a list of nodes\n",
    "    \"\"\"\n",
    "\n",
    "    G = list(Graph.edges())\n",
    "    G = pd.DataFrame(G, columns=['source', 'target'])\n",
    "    \n",
    "    # Step 1. Select random source node\n",
    "    source = random.choice(np.unique(G['source']))\n",
    "    \n",
    "    # Step 2. Get an instance of g from G by sampling edges  \n",
    "    g = G.copy().loc[np.random.uniform(0,1,G.shape[0]) < p]\n",
    "\n",
    "    # Step 3. Construct reverse reachable set of the random source node\n",
    "    new_nodes, RRS0 = [source], [source]   \n",
    "    while new_nodes:\n",
    "        \n",
    "        # Limit to edges that flow into the source node\n",
    "        temp = g.loc[g['target'].isin(new_nodes)]\n",
    "\n",
    "        # Extract the nodes flowing into the source node\n",
    "        temp = temp['source'].tolist()\n",
    "\n",
    "        # Add new set of in-neighbors to the RRS\n",
    "        RRS = list(set(RRS0 + temp))\n",
    "\n",
    "        # Find what new nodes were added\n",
    "        new_nodes = list(set(RRS) - set(RRS0))\n",
    "\n",
    "        # Reset loop variables\n",
    "        RRS0 = RRS[:]\n",
    "\n",
    "    return(RRS)\n",
    "\n",
    "def Reverse_Influence_Sampling(Graph, percentage_of_nodes,p=0.1,mc=1000):\n",
    "    \"\"\"\n",
    "    Inputs: Graph:  NetworkX graph\n",
    "            percentage_of_nodes:  Size of seed set\n",
    "            p:  Disease propagation probability\n",
    "            mc: Number of RRSs to generate\n",
    "    Return: A seed set of nodes as an approximate solution to the IM problem\n",
    "    \"\"\"\n",
    "    G = list(Graph.edges())\n",
    "    G = pd.DataFrame(G, columns=['source', 'target'])\n",
    "    num_nodes = Graph.number_of_nodes()\n",
    "    k = int(num_nodes * percentage_of_nodes)\n",
    "    \n",
    "    # Step 1. Generate the collection of random RRSs\n",
    "    # start_time = time.time()\n",
    "    R = [get_RRS(Graph,p) for _ in range(mc)]\n",
    "\n",
    "    # Step 2. Choose nodes that appear most often (maximum coverage greedy algorithm)\n",
    "    # SEED, timelapse = [], []\n",
    "    SEED = []\n",
    "    # print(f\"Number of RRSs generated: {len(R)}\")\n",
    "    # print(f\"Size of seed set: {k}\")\n",
    "    for _ in range(k):\n",
    "        \n",
    "        # Find node that occurs most often in R and add to seed set\n",
    "        flat_list = [item for sublist in R for item in sublist]\n",
    "\n",
    "        if not flat_list:\n",
    "            # print(\"Percentage of nodes: \", percentage_of_nodes)\n",
    "            # print(\"No more nodes available to select.\")\n",
    "            # print(f\"Final seed set: {sorted(SEED)}\")\n",
    "            break  \n",
    "        seed = Counter(flat_list).most_common()[0][0]\n",
    "        SEED.append(int(seed))\n",
    "        \n",
    "        # Remove RRSs containing last chosen seed \n",
    "        R = [rrs for rrs in R if seed not in rrs]\n",
    "        \n",
    "        # Record Time\n",
    "        # timelapse.append(time.time() - start_time)\n",
    "    \n",
    "    return(sorted(SEED))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115ccff5",
   "metadata": {},
   "source": [
    "## CELF Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02625ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cost(A,c,node_idx_dict):\n",
    "    res = 0\n",
    "    for a in A:\n",
    "        res += c[node_idx_dict[a]]\n",
    "    return res\n",
    "\n",
    "def min_remaining_cost(remaining_nodes,c,node_idx_dict):\n",
    "    res = np.max(c)\n",
    "    for a in remaining_nodes:\n",
    "        res = min(res,c[node_idx_dict[a]])\n",
    "    return res\n",
    "\n",
    "def Reward(G,A,num_iterations,propagation_model,Unif_influence_prob=True,influence_prob=0.25):\n",
    "    Expected_reward = 0\n",
    "    if propagation_model==\"LT\":\n",
    "        for i in range(num_iterations):\n",
    "            Expected_reward += len(linear_threshold(G,A)) ### Fill in the third argument here\n",
    "\n",
    "    elif propagation_model==\"IC\":\n",
    "        if Unif_influence_prob:\n",
    "            Influence_probabilities = Uniform_influence_prob(G,influence_prob)\n",
    "            # print(1)\n",
    "            # print(type(Influence_probabilities))\n",
    "            for i in range(num_iterations):\n",
    "                Expected_reward += len(Independent_Cascade_model(G,A,Influence_probabilities))\n",
    "\n",
    "        else:\n",
    "            # print(0)\n",
    "            Influence_probabilities = Weighted_cascade(G)\n",
    "            # print(type(Influence_probabilities))\n",
    "            for i in range(num_iterations):\n",
    "                Expected_reward += len(Independent_Cascade_model(G,A,Influence_probabilities))\n",
    "\n",
    "    Expected_reward /= num_iterations\n",
    "    return Expected_reward\n",
    "\n",
    "def Lazy_Forward(G,c,B,num_iterations,propagation_model,Unif_influence_prob=True,influence_prob=0.25):### Need to make sure that cost c has proper index. Check whether the index is the node value itself or should it be computed based on G.nodes (like node_idx_dict in Independent_Cascade_model() function)\n",
    "    nodes = G.nodes\n",
    "    node_idx_dict = {x: idx for idx, x in enumerate(G.nodes)}\n",
    "    num_of_nodes = G.number_of_nodes()\n",
    "    A = set()\n",
    "    remaining_nodes = nodes-A\n",
    "    incremental_reward = {x:-1 for x in nodes}\n",
    "    init_flag = True\n",
    "    curr_reward = 0\n",
    "    # print(\"remaining_nodes:\\n\",remaining_nodes)\n",
    "    index = 0\n",
    "    while min_remaining_cost(remaining_nodes,c,node_idx_dict)<=B-Cost(A,c,node_idx_dict):\n",
    "        remaining_nodes = remaining_nodes - set([n for n in remaining_nodes if c[node_idx_dict[n]]>B-Cost(A,c,node_idx_dict)])\n",
    "        # print(\"iteration: \",index)\n",
    "        # index += 1\n",
    "        # print(\"remaining_nodes:\\n\",remaining_nodes)\n",
    "        if init_flag:\n",
    "            for s in remaining_nodes:\n",
    "                incremental_reward[s] = (Reward(G,A|set([s]),num_iterations,propagation_model,Unif_influence_prob,influence_prob)-curr_reward)/c[node_idx_dict[s]]\n",
    "\n",
    "            init_flag = False\n",
    "\n",
    "        else:\n",
    "            keys_list = list(incremental_reward.keys())\n",
    "            for idx in range(len(keys_list)):\n",
    "                incremental_reward[keys_list[idx]] = (Reward(G,A|set([keys_list[idx]]),num_iterations,propagation_model,Unif_influence_prob,influence_prob)-curr_reward)/c[node_idx_dict[s]]\n",
    "                if idx<len(keys_list)-1 and incremental_reward[keys_list[idx]]>incremental_reward[keys_list[idx+1]]:\n",
    "                    break\n",
    "\n",
    "        incremental_reward = dict(sorted(incremental_reward.items(), key=lambda item: item[1], reverse=True))\n",
    "        print(f\"size: {len(incremental_reward)}\")\n",
    "        print(\"incremental reward,\\n\",{key:vals*c[node_idx_dict[key]] for key, vals in incremental_reward.items()})\n",
    "        print(f\"curr_reward: {curr_reward}\")\n",
    "        max_incremental_reward_node = list(incremental_reward.keys())[0]\n",
    "        A = A|set([max_incremental_reward_node])\n",
    "        curr_reward = curr_reward + incremental_reward[max_incremental_reward_node]*c[node_idx_dict[max_incremental_reward_node]]\n",
    "        remaining_nodes = remaining_nodes-set([max_incremental_reward_node])\n",
    "        incremental_reward.pop(max_incremental_reward_node)\n",
    "        # print(f\"A: {A}\")\n",
    "        # print(f\"curr_reward: {curr_reward}\")\n",
    "\n",
    "    return A, curr_reward\n",
    "\n",
    "def CELF(G, costs, percentage_of_nodes,num_iterations,propagation_model,Unif_influence_prob=True,influence_prob=0.25):\n",
    "    max_cost = int(G.number_of_nodes()*percentage_of_nodes/100)\n",
    "    A_UC, R_UC = Lazy_Forward(G,np.ones(G.number_of_nodes()),max_cost,num_iterations,propagation_model,Unif_influence_prob,influence_prob)\n",
    "    A_CB, R_CB = Lazy_Forward(G,costs,max_cost,num_iterations,propagation_model,Unif_influence_prob,influence_prob)\n",
    "\n",
    "    if R_UC>R_CB:\n",
    "        A_opt, R_opt =  A_UC, R_UC\n",
    "\n",
    "    else:\n",
    "        A_opt, R_opt = A_CB, R_CB\n",
    "\n",
    "    return A_opt, R_opt, R_CB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab64e6f",
   "metadata": {},
   "source": [
    "## Baseline Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbaea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Uniform_Sampling(G, percentage_of_nodes):\n",
    "    k = int((percentage_of_nodes/100)*G.number_of_nodes())\n",
    "    seed_nodes = np.random.choice(G.nodes,size=k,replace=False)\n",
    "    return seed_nodes\n",
    "\n",
    "def Degree_Centrality(G,percentage_of_nodes):\n",
    "    k = int((percentage_of_nodes/100)*G.number_of_nodes())\n",
    "    degree_dict = dict(G.degree())\n",
    "    sorted_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    seed_nodes = [node for node, _ in sorted_nodes[:k]]\n",
    "    return seed_nodes\n",
    "\n",
    "def Eigen_vector_Centrality(G,percentage_of_nodes):\n",
    "    k = int((percentage_of_nodes/100)*G.number_of_nodes())\n",
    "    eigvec_centrality = nx.eigenvector_centrality(G)\n",
    "    sorted_nodes = sorted(eigvec_centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "    seed_nodes = [node for node, _ in sorted_nodes[:k]]\n",
    "    return seed_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379dc9a9",
   "metadata": {},
   "source": [
    "## Example Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1272d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 100\n",
    "mean_ic_bg = []\n",
    "mean_lt_bg = []\n",
    "mean_ic_eg = []\n",
    "mean_lt_eg = []\n",
    "mean_ic_fbg = []\n",
    "mean_lt_fbg = []\n",
    "mean_ic_bcg = []\n",
    "mean_lt_bcg = []\n",
    "percentages = np.arange(0.00, 0.51, 0.01)\n",
    "\n",
    "for pct in percentages:\n",
    "    seed_set_bg = Reverse_Influence_Sampling(BG, pct)\n",
    "    seed_set_eg = Reverse_Influence_Sampling(EG, pct)\n",
    "    seed_set_fbg = Reverse_Influence_Sampling(fbG, pct)\n",
    "    seed_set_bcg = Reverse_Influence_Sampling(BcG, pct)\n",
    "    results_ic_bg_sim = []\n",
    "    results_lt_bg_sim = []\n",
    "    results_ic_eg_sim = []\n",
    "    results_lt_eg_sim = []\n",
    "    results_ic_fbg_sim = []\n",
    "    results_lt_fbg_sim = []\n",
    "    results_ic_bcg_sim = []\n",
    "    results_lt_bcg_sim = []\n",
    "    for sim in range(num_simulations):\n",
    "        linear_seeds_bg = linear_threshold(BG, seed_set_bg)\n",
    "        linear_seeds_eg = linear_threshold(EG, seed_set_eg)\n",
    "        linear_seeds_fbg = linear_threshold(fbG, seed_set_fbg)\n",
    "        linear_seeds_bcg = linear_threshold(BcG, seed_set_bcg)\n",
    "\n",
    "        independent_seeds_bg = Independent_Cascade_model(BG, seed_set_bg, Uniform_influence_prob(BG, 0.2))\n",
    "        independent_seeds_eg = Independent_Cascade_model(EG, seed_set_eg, Uniform_influence_prob(EG, 0.2))\n",
    "        independent_seeds_fbg = Independent_Cascade_model(fbG, seed_set_fbg, Uniform_influence_prob(fbG, 0.2))\n",
    "        independent_seeds_bcg = Independent_Cascade_model(BcG, seed_set_bcg, Uniform_influence_prob(BcG, 0.2))\n",
    "\n",
    "        results_ic_bg_sim.append(len(independent_seeds_bg) / len(BG.nodes()))\n",
    "        results_lt_bg_sim.append(len(linear_seeds_bg) / len(BG.nodes()))\n",
    "        results_ic_eg_sim.append(len(independent_seeds_eg) / len(EG.nodes()))\n",
    "        results_lt_eg_sim.append(len(linear_seeds_eg) / len(EG.nodes()))\n",
    "        results_ic_fbg_sim.append(len(independent_seeds_fbg) / len(fbG.nodes()))\n",
    "        results_lt_fbg_sim.append(len(linear_seeds_fbg) / len(fbG.nodes()))\n",
    "        results_ic_bcg_sim.append(len(independent_seeds_bcg) / len(BcG.nodes()))\n",
    "        results_lt_bcg_sim.append(len(linear_seeds_bcg) / len(BcG.nodes()))\n",
    "    # Calculate the mean results for each model across all simulations\n",
    "    mean_ic_bg.append(np.mean(results_ic_bg_sim))\n",
    "    mean_lt_bg.append(np.mean(results_lt_bg_sim))\n",
    "    mean_ic_eg.append(np.mean(results_ic_eg_sim))\n",
    "    mean_lt_eg.append(np.mean(results_lt_eg_sim))\n",
    "    mean_ic_fbg.append(np.mean(results_ic_fbg_sim))\n",
    "    mean_lt_fbg.append(np.mean(results_lt_fbg_sim))\n",
    "    mean_ic_bcg.append(np.mean(results_ic_bcg_sim))\n",
    "    mean_lt_bcg.append(np.mean(results_lt_bcg_sim))\n",
    "    \n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(percentages, mean_ic_bg, label='Barabasi IC')\n",
    "plt.plot(percentages, mean_lt_bg, label='Barabasi LT')\n",
    "plt.plot(percentages, mean_ic_eg, label='Erdos IC')\n",
    "plt.plot(percentages, mean_lt_eg, label='Erdos LT')\n",
    "plt.plot(percentages, mean_ic_fbg, label='Facebook IC')\n",
    "plt.plot(percentages, mean_lt_fbg, label='Facebook LT')\n",
    "plt.plot(percentages, mean_ic_bcg, label='Bitcoin IC')\n",
    "plt.plot(percentages, mean_lt_bcg, label='Bitcoin LT')\n",
    "plt.plot(percentages, percentages, label='Seed Set Percentage', linestyle='--', color='gray')\n",
    "plt.xlabel('Seed Set Percentage')\n",
    "plt.ylabel('Fraction of Activated Nodes')\n",
    "plt.title('Influence Spread vs. Seed Set Percentage (Reverse Influence Sampling)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the mean results to a CSV file\n",
    "results_df = pd.DataFrame({\n",
    "    'Percentage': percentages,\n",
    "    'Barabasi_IC_RIS': mean_ic_bg,\n",
    "    'Barabasi_LT_RIS': mean_lt_bg,\n",
    "    'Erdos_IC_RIS': mean_ic_eg,\n",
    "    'Erdos_LT_RIS': mean_lt_eg,\n",
    "    'Facebook_IC_RIS': mean_ic_fbg,\n",
    "    'Facebook_LT_RIS': mean_lt_fbg,\n",
    "    'Bitcoin_IC_RIS': mean_ic_bcg,\n",
    "    'Bitcoin_LT_RIS': mean_lt_bcg\n",
    "})\n",
    "results_df.to_csv('RIS_results.csv', index=False)\n",
    "\n",
    "\n",
    "p_vals = np.linspace(0,50,num=51)\n",
    "Spread_LT = np.zeros_like(p_vals)\n",
    "num_iterations_MC_simulations = 100\n",
    "for idx in range(p_vals.size):\n",
    "    p = p_vals[idx]\n",
    "    seed_nodes_CELF_LT, R_LT, _ = CELF(BG,np.ones(BG.number_of_nodes()),percentage_of_nodes=p,num_iterations=num_iterations_MC_simulations,propagation_model=\"LT\")\n",
    "    # for i in range(num_iterations_MC_simulations):\n",
    "    #     Spread_LT[idx] += len(linear_threshold(sfg,seed_nodes_CELF_LT))\n",
    "    #     Spread_IC[idx] += len(Independent_Cascade_model(sfg,seed_nodes_CELF_IC,influence_probabilities=Uniform_influence_prob(sfg,0.25)))\n",
    "\n",
    "    Spread_LT[idx] = R_LT \n",
    "    \n",
    "    print(f\"p={p} done...\")\n",
    "\n",
    "Spread_IC = np.zeros_like(p_vals)\n",
    "for idx in range(p_vals.size):\n",
    "    p = p_vals[idx]\n",
    "    seed_nodes_CELF_IC, R_IC, _ = CELF(BG,np.ones(BG.number_of_nodes()),percentage_of_nodes=p,num_iterations=num_iterations_MC_simulations,propagation_model=\"IC\",Unif_influence_prob=True,influence_prob=0.2)\n",
    "\n",
    "    Spread_IC[idx] = R_IC\n",
    "    print(f\"p={p} done...\")\n",
    "\n",
    "p_vals = np.linspace(0,50,num=51)\n",
    "Spread_LT_Barabasi_Rand_Sampling = np.zeros_like(p_vals)\n",
    "num_iterations_MC_simulations = 100\n",
    "for idx in range(p_vals.size):\n",
    "    p = p_vals[idx]\n",
    "    seed_nodes_Rand_Sample = Uniform_Sampling(BG,p)\n",
    "    Spread_LT_Barabasi_Rand_Sampling[idx] = Reward(BG,seed_nodes_Rand_Sample,num_iterations=num_iterations_MC_simulations,propagation_model=\"LT\")\n",
    "    \n",
    "    print(f\"p={p} done...\")\n",
    "\n",
    "Spread_IC_Barabasi_Rand_Sampling = np.zeros_like(p_vals)\n",
    "p_vals = np.linspace(0,50,num=51)\n",
    "num_iterations_MC_simulations = 100\n",
    "for idx in range(p_vals.size):\n",
    "    p = p_vals[idx]\n",
    "    seed_nodes_Rand_Sample = Uniform_Sampling(BG,p)\n",
    "    Spread_IC_Barabasi_Rand_Sampling[idx] = Reward(BG,seed_nodes_Rand_Sample,num_iterations=num_iterations_MC_simulations,propagation_model=\"IC\",Unif_influence_prob=True,influence_prob=0.2)\n",
    "    \n",
    "    print(f\"p={p} done...\")\n",
    "\n",
    "percentages = list(range(1, 51))\n",
    "ic_spread_greedy = []\n",
    "ic_spread_eigen = []\n",
    "lt_spread_greedy = []\n",
    "lt_spread_eigen = []\n",
    "N = BG.number_of_nodes()\n",
    "influence_probabilities = np.zeros((N, N))\n",
    "for u, v in BG.edges():\n",
    "    influence_probabilities[u, v] = 0.2\n",
    "    influence_probabilities[v, u] = 0.2\n",
    "\n",
    "for p in percentages:\n",
    "    k = int((p / 100) * BG.number_of_nodes())\n",
    "\n",
    "    greedy_seeds = greedy_optimizer(BG, k, lambda G, S: estimate_spread_ic(G, S, simulations=10))\n",
    "    eigen_seeds = Eigen_vector_Centrality(BG, p)\n",
    "\n",
    "    ic_spread_greedy.append(estimate_spread_ic(BG, greedy_seeds, simulations=10) / BG.number_of_nodes())\n",
    "    ic_spread_eigen.append(estimate_spread_ic(BG, eigen_seeds, simulations=10) / BG.number_of_nodes())\n",
    "\n",
    "    lt_spread_greedy.append(estimate_spread_lt(BG, greedy_seeds, simulations=10) / BG.number_of_nodes())\n",
    "    lt_spread_eigen.append(estimate_spread_lt(BG, eigen_seeds, simulations=10) / BG.number_of_nodes())\n",
    "\n",
    "#Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(percentages, ic_spread_greedy, label=\"Greedy - IC\", marker='o', color='orange')\n",
    "plt.plot(percentages, ic_spread_eigen, label=\"Eigenvector - IC\", marker='^', color='green')\n",
    "plt.plot(percentages, lt_spread_greedy, label=\"Greedy - LT\", marker='s', color='blue')\n",
    "plt.plot(percentages, lt_spread_eigen, label=\"Eigenvector - LT\", marker='d', color='red')\n",
    "plt.xlabel(\"Percentage of seed nodes (%)\")\n",
    "plt.ylabel(\"Fraction of activated nodes\")\n",
    "plt.title(\"Greedy vs Eigenvector: Influence Spread (IC & LT)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
